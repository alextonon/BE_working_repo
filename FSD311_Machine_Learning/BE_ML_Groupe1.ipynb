{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2641a4",
   "metadata": {},
   "source": [
    "# BE - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dccda26",
   "metadata": {},
   "source": [
    "### Données\n",
    "\n",
    "Le dataset est un dataset UCML, on pourrait utiliser le module *ucmlrepo* et la methode *fetch_ucirepo* associée, mais le chargement des données est long (~2 minutes). On privilégiera donc un téléchargement et des méthodes de lecture manuelles, les données ne faisant que 600Ko\n",
    "\n",
    "Le dataset a été construit pour prédire la target *income* qui comporte deux valeurs (>50K, <=50K) à partir de 14 features. Cette construction en \"classes\" nous impose de travailler avec des classifiers plutôt qu'avec des regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb107c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = pd.read_table(\"data/adult.names\", sep=\":\", skiprows=96, header=None)[0].tolist()\n",
    "names.append(\"income\") # La target n'est pas dans le fichier des noms\n",
    "adult = pd.read_csv(\"data/adult.data\", sep=\", \", names=names, engine='python', index_col=False) # Le engine permet juste d'éviter un warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c02c7a7",
   "metadata": {},
   "source": [
    "### Nettoyage et Pre processing\n",
    "\n",
    "Avant de mettre en place des modèles il nous fait nettoyer un peu les données pour pouvoir les exploiter. En particulier, nous aimerions supprimer les *valuers manquantes* ainsi que les *outliers*. En mettant en place les algo classiques, nous nous sommes vite rendu compte d'un problème : la plupart des catégories sont encodées sous forme de texte. Ainsi les algo ne peuvent fonctionner. C'est la raison pour laquelle nous allons mettre en place cette pipeline de nettoyage :\n",
    "- **Suppression des #NA :** Les NA étant encodés par des \"?\" (il n'y a pas de NA sinon) nous allons supprimer chaque lignes qui en comportent\n",
    "- **Encodage des colonnes :** Nous allons encoder chaque colonnes de la façon suivante : chaque valeur unique se voit attribuer un entier. Nous n'avons que peu de recul pour l'instant et ne savons pas si c'est pertinent\n",
    "- **Normalisation :** POur contrer l'effet du nombre e colonnes, nous allons normaliser celles ci\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd59ae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass : 1836 NA\n",
      "occupation : 1843 NA\n",
      "native-country : 583 NA\n"
     ]
    }
   ],
   "source": [
    "#### Suppression des NA ####\n",
    "adult.replace(\"?\", pd.NA, inplace=True)  # Les NA sont codés par des \"?\"\n",
    "\n",
    "for key in adult.keys():\n",
    "    n_na = adult[key].isna().sum()\n",
    "    if n_na > 0:\n",
    "        print(f\"{key} : {n_na} NA\")\n",
    "\n",
    "adult.dropna(inplace=True)  # On supprime les lignes avec NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19099d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Encodage des colonnes ####\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {}\n",
    "for column in adult.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    adult[column] = le.fit_transform(adult[column])\n",
    "    label_encoders[column] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ccdb44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Normalisation ####\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "for column in adult.columns:\n",
    "    if column != \"income\":  # On ne normalise pas la target\n",
    "        adult[[column]] = scaler.fit_transform(adult[[column]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eefc84e",
   "metadata": {},
   "source": [
    "### Data Vis\n",
    "\n",
    "Projet : faire une acp ? regarder un peu comment on se sort des 15 variables degeus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054f5f2",
   "metadata": {},
   "source": [
    "### Tests sur les outliers ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f6f51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes après suppression des NA : 30162\n",
      "Nombre de lignes après suppression des outliers : 27145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "print(f\"Nombre de lignes après suppression des NA : {adult.shape[0]}\")\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "outliers = iso_forest.fit_predict(adult.drop(columns=[\"income\"]))\n",
    "mask = outliers != -1\n",
    "adult = adult[mask]\n",
    "\n",
    "print(f\"Nombre de lignes après suppression des outliers : {adult.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93111c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90      4141\n",
      "           1       0.72      0.60      0.66      1288\n",
      "\n",
      "    accuracy                           0.85      5429\n",
      "   macro avg       0.80      0.76      0.78      5429\n",
      "weighted avg       0.84      0.85      0.85      5429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = adult.drop(columns=[\"income\"])\n",
    "y = adult[\"income\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f43560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91      4141\n",
      "           1       0.75      0.64      0.69      1288\n",
      "\n",
      "    accuracy                           0.87      5429\n",
      "   macro avg       0.82      0.79      0.80      5429\n",
      "weighted avg       0.86      0.87      0.86      5429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "y_pred = bst.predict(dtest)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred] \n",
    "\n",
    "print(classification_report(y_test, y_pred_binary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
